{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chat\n",
    "`pne.chat()` is an awesome function, you can use **tools, formatted output, different llm** in this function. \n",
    "\n",
    "`pne.chat()` integrate the ability of [litellm](https://github.com/BerriAI/litellm). It means you can call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs). Now let's take a look at how to use it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db2c1ac319e2854b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat like OpenAI\n",
    "You can use `pne.chat()` to chat like openai. OpenAI chat API document: [https://platform.openai.com/docs/api-reference/chat](https://platform.openai.com/docs/api-reference/chat)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab40a7999dea9f0f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a helpful assistant designed to assist you with any questions or tasks you may have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response: str = pne.chat(messages=messages)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T18:07:49.926776700Z",
     "start_time": "2023-12-06T18:07:47.382356800Z"
    }
   },
   "id": "87ea02d652933cfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, you can only pass a string to `pne.chat()`, it will automatically convert it to the OpenAI format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a591775c9503693c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My knowledge is up to date as of March 2021. Any events or developments occurring after that date would not be included in my responses. If you're asking for any recent information or updates, I recommend checking the latest sources as my information might not be current.\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response = pne.chat(\n",
    "    messages=\"When is your knowledge up to?\",\n",
    "    model=\"gpt-4-1106-preview\"\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T19:12:18.305425200Z",
     "start_time": "2023-12-06T19:12:11.264168300Z"
    }
   },
   "id": "b54acb1bd5360e53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Return type\n",
    "`pne.chat()` return string by default.\n",
    "\n",
    "If you want to do more complex thing, metadata is important. You can use `return_raw_response=True` to get the raw response wrapped by `pne.AssistantMessage`. Metadata will store in `pne.AssistantMessage.additional_kwargs`.\n",
    "\n",
    "\n",
    "> About `pne.AssistantMessage`, you can see [here](modules/schema.md#Schema)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986436f7d728cb57"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI assistant here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "{'id': 'chatcmpl-8UK0tfwlkixWyaxKJ2XWNGMVGFPo0', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'I am an AI assistant here to help you with any questions or tasks you may have. How can I assist you today?', 'role': 'assistant'}}], 'created': 1702237461, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': {'completion_tokens': 25, 'prompt_tokens': 20, 'total_tokens': 45}, '_response_ms': 2492.372}\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response: pne.AssistantMessage = pne.chat(\"Who are you?\", return_raw_response=True)\n",
    "print(response.content) # response string\n",
    "print(response.additional_kwargs) # metadata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T19:44:23.789267100Z",
     "start_time": "2023-12-10T19:44:21.287383900Z"
    }
   },
   "id": "dfd2be8184fde7c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using any model\n",
    "You can call 100+ LLMs using the same Input/Output Format(OpenAI format) in `pne.chat()`. The follow example show how to use `claude-2`, make sure you have key ANTHROPIC_API_KEY."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90d35de8282c9a20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import promptulate as pne\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response = pne.chat(messages=messages, model=\"claude-2\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c2e222f3e47af2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HuggingFace\n",
    "This example show how to use HuggingFace LLMs in `pne.chat()`. Make sure you have key HUGGINGFACE_API_KEY."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37c2a4cf5094bfa6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import promptulate as pne\n",
    "\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response = pne.chat(\n",
    "    messages=messages,\n",
    "    model=\"huggingface/WizardLM/WizardCoder-Python-34B-V1.0\",\n",
    "    api_base=\"https://my-endpoint.huggingface.cloud\"\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b7f930730c8edc4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Azure OpenAI\n",
    "This example show how to use Azure OpenAI LLMs in `pne.chat()`. Make sure you have relevant key."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aae7346115ec99e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import promptulate as pne\n",
    "\n",
    "os.environ[\"AZURE_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_API_BASE\"] = \"\"\n",
    "os.environ[\"AZURE_API_VERSION\"] = \"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response = pne.chat(\n",
    "    messages=messages,\n",
    "    model=\"azure/<your_deployment_name>\",\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bc66280c7f75187"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom LLM\n",
    "You can use `pne.llms.BaseLLM` to create your own LLM. The follow example show how to create a custom LLM and use it in `pne.chat()`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40ec37df0dc9bab5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a helpful assistant designed to provide information, answer questions, and assist with various tasks. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class CustomLLM(pne.llms.BaseLLM):\n",
    "    \"\"\"\n",
    "    This is a custom LLM, here we wrap OpenAI API to implement it.\n",
    "    \"\"\"\n",
    "    llm_type: str = \"custom_llm\"\n",
    "    llm = pne.ChatOpenAI()\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _predict(self, prompts: pne.MessageSet, *args, **kwargs) -> Optional[\n",
    "        pne.AssistantMessage]:\n",
    "        return self.llm.predict(prompts, *args, **kwargs)\n",
    "\n",
    "    def __call__(self, prompt: str, *args, **kwargs):\n",
    "        return self.llm(prompt, *args, **kwargs)\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response = pne.chat(\n",
    "    messages=messages,\n",
    "    custom_llm=CustomLLM(),\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T19:03:47.080442300Z",
     "start_time": "2023-12-06T19:03:45.078761200Z"
    }
   },
   "id": "aa746c259a9edd4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output Format\n",
    "The output of LLM has strong uncertainty. Pne provide the ability to get a formatted object by LLM. The following example shows that if LLM strictly returns you an array listing all provinces in China. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f82e8d518c46350"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong', 'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan', 'Hubei', 'Hunan', 'Jiangsu', 'Jiangxi', 'Jilin', 'Liaoning', 'Ningxia', 'Qinghai', 'Shaanxi', 'Shandong', 'Shanghai', 'Shanxi', 'Sichuan', 'Tianjin', 'Tibet', 'Xinjiang', 'Yunnan', 'Zhejiang']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import promptulate as pne\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    provinces: List[str]= Field(description=\"All provinces in China\")\n",
    "\n",
    "response: LLMResponse = pne.chat(\"Please tell me all provinces in China.\", output_schema=LLMResponse)\n",
    "print(response.provinces)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T20:26:38.324581100Z",
     "start_time": "2023-12-10T20:26:33.640080200Z"
    }
   },
   "id": "fa4efacb171d3dc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, `pne.chat()` return a LLMResponse object. The value of provinces is all provinces in China. If you are building a complex Agent project, formatting output is a necessary measure to improve system robustness."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f8c1b99869e96ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using tool\n",
    "You can use `pne.tools` to add some tools to chat. Now we have `pne.tools.duckduckgo.DuckDuckGoTool()`, it can help you to get the answer from DuckDuckGo.\n",
    "\n",
    "> âš  There are some tiny bugs if you use tools, we are fixing it. We are ready to release the first version of `pne.tools` in the next version."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "814b3228c2da3a71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tools = [pne.tools.duckduckgo.DuckDuckGoTool()]\n",
    "response = pne.chat(\n",
    "    messages=\"What's the temperature in Shanghai tomorrow?\",\n",
    "    tools=tools\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7640390bf6a79c07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Streaming\n",
    "`pne.chat()` support streaming, you can use `pne.chat()` to chat with your assistant in real time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3475e266c1c31343"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      " am\n",
      " a\n",
      " virtual\n",
      " assistant\n",
      " designed\n",
      " to\n",
      " provide\n",
      " information\n",
      " and\n",
      " assistance\n",
      ".\n",
      " Is\n",
      " there\n",
      " something\n",
      " specific\n",
      " you\n",
      " would\n",
      " like\n",
      " help\n",
      " with\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response = pne.chat(\"Who are you?\", stream=True)\n",
    "\n",
    "for chuck in response:\n",
    "    print(chuck)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T20:06:55.543024400Z",
     "start_time": "2023-12-10T20:06:53.035473Z"
    }
   },
   "id": "b2cf5a0c878dffe9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`pne.chat()` by stream will return an iterator, you can use `next()` or `for each` to get the response.\n",
    "\n",
    "If you want to get metadata, you can use `return_raw_response=True` to get the raw response wrapped by `pne.AssistantMessage`. Metadata will store in `pne.AssistantMessage.additional_kwargs`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b66f83fb7b085d15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response = pne.chat(\"Who are you?\", stream=True, return_raw_response=True)\n",
    "\n",
    "for chuck in response:\n",
    "    print(chuck.content)\n",
    "    print(chuck.additional_kwargs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d782fb41fe96150"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve && RAG\n",
    "**RAG(Retrieval-Augmented Generation)** is a important data retrieve method. You can use `pne.chat()` to retrieve data from your database.\n",
    "\n",
    "You can use lots of methods to retrieve data, pne plan to support the following source:\n",
    "- [ ] VectorStore Retrieval\n",
    "- [ ] Relational Database Retrieval\n",
    "- [ ] Web Search Retrieval\n",
    "- [ ] Knowledge Graph Retrieval\n",
    "- [ ] Document Retrieval\n",
    "    - [ ] PDF Retrieval\n",
    "    - [ ] Docx Retrieval\n",
    "    - [ ] CSV/Excel Retrieval\n",
    "- [ ] Image Retrieval\n",
    "\n",
    "ðŸŒŸ**We are currently building infrastructure, please stay tuned!**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a11f1f73fd315d79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
