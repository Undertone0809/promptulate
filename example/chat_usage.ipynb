{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> All your need is a `pne.chat()` function.\n",
    "\n",
    "# Chat\n",
    "`pne.chat()` is an awesome function, you can use **tools, formatted output, different llm** in this function. \n",
    "\n",
    "## Best Practice\n",
    "\n",
    "Here are some tips for using `pne.chat()`. Even though pne provides many modules, in 90% of LLM application development scenarios, you only need to use the pne.chat () function, so you only need to start with chat to understand the use of pne, and when you need to use additional modules, you can learn more about the features and use of other modules.\n",
    "\n",
    "`pne.chat()` integrate the ability of [litellm](https://github.com/BerriAI/litellm). It means you can call all LLM APIs using the OpenAI format. Use Bedrock, Azure, OpenAI, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, Replicate (100+ LLMs). Now let's take a look at how to use it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db2c1ac319e2854b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat like OpenAI\n",
    "You can use `pne.chat()` to chat like openai. OpenAI chat API document: [https://platform.openai.com/docs/api-reference/chat](https://platform.openai.com/docs/api-reference/chat)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab40a7999dea9f0f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a helpful assistant designed to provide information and assistance to users like you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response: str = pne.chat(messages=messages, model=\"gpt-4-turbo\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:31:37.359443Z",
     "start_time": "2024-04-25T10:31:30.702171Z"
    }
   },
   "id": "87ea02d652933cfa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, you can only pass a string to `pne.chat()`, it will automatically convert it to the OpenAI format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a591775c9503693c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My knowledge is up to date as of March 2021. Any events or developments occurring after that date would not be included in my responses. If you're asking for any recent information or updates, I recommend checking the latest sources as my information might not be current.\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response = pne.chat(\n",
    "    messages=\"When is your knowledge up to?\",\n",
    "    model=\"gpt-4-turbo\"\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T19:12:18.305425200Z",
     "start_time": "2023-12-06T19:12:11.264168300Z"
    }
   },
   "id": "b54acb1bd5360e53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Return type\n",
    "`pne.chat()` return string by default.\n",
    "\n",
    "If you want to do more complex thing, metadata is important. You can use `return_raw_response=True` to get the raw response wrapped by `pne.AssistantMessage`. Metadata will store in `pne.AssistantMessage.additional_kwargs`.\n",
    "\n",
    "\n",
    "> About `pne.AssistantMessage`, you can see [here](modules/schema.md#Schema)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986436f7d728cb57"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI assistant here to help you with any questions or tasks you may have. How can I assist you today?\n",
      "{'id': 'chatcmpl-8UK0tfwlkixWyaxKJ2XWNGMVGFPo0', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'I am an AI assistant here to help you with any questions or tasks you may have. How can I assist you today?', 'role': 'assistant'}}], 'created': 1702237461, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': {'completion_tokens': 25, 'prompt_tokens': 20, 'total_tokens': 45}, '_response_ms': 2492.372}\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response: pne.AssistantMessage = pne.chat(messages=\"Who are you?\", model=\"gpt-4-turbo\",\n",
    "                                          return_raw_response=True)\n",
    "print(response.content)  # response string\n",
    "print(response.additional_kwargs)  # metadata"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T19:44:23.789267100Z",
     "start_time": "2023-12-10T19:44:21.287383900Z"
    }
   },
   "id": "dfd2be8184fde7c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using any model\n",
    "You can call 100+ LLMs using the same Input/Output Format(OpenAI format) in `pne.chat()`. The follow example show how to use `claude-2`, make sure you have key ANTHROPIC_API_KEY."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90d35de8282c9a20"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import promptulate as pne\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response = pne.chat(messages=messages, model=\"claude-2\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c2e222f3e47af2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HuggingFace\n",
    "This example show how to use HuggingFace LLMs in `pne.chat()`. Make sure you have key HUGGINGFACE_API_KEY."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37c2a4cf5094bfa6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import promptulate as pne\n",
    "\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"your-api-key\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response = pne.chat(\n",
    "    messages=messages,\n",
    "    model=\"huggingface/WizardLM/WizardCoder-Python-34B-V1.0\",\n",
    "    model_config={\"api_base\": \"https://my-endpoint.huggingface.cloud\"}\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b7f930730c8edc4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Azure OpenAI\n",
    "This example show how to use Azure OpenAI LLMs in `pne.chat()`. Make sure you have relevant key."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4aae7346115ec99e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import promptulate as pne\n",
    "\n",
    "os.environ[\"AZURE_API_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_API_BASE\"] = \"\"\n",
    "os.environ[\"AZURE_API_VERSION\"] = \"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response = pne.chat(\n",
    "    messages=messages,\n",
    "    model=\"azure/<your_deployment_name>\",\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bc66280c7f75187"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom LLM\n",
    "You can use `pne.llms.BaseLLM` to create your own LLM. The follow example show how to create a custom LLM and use it in `pne.chat()`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40ec37df0dc9bab5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a helpful assistant designed to provide information, answer questions, and assist with various tasks. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class CustomLLM(pne.llms.BaseLLM):\n",
    "    \"\"\"\n",
    "    This is a custom LLM, here we wrap OpenAI API to implement it.\n",
    "    \"\"\"\n",
    "    llm_type: str = \"custom_llm\"\n",
    "    llm = pne.ChatOpenAI()\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _predict(self, prompts: pne.MessageSet, *args, **kwargs) -> Optional[\n",
    "        pne.AssistantMessage]:\n",
    "        return self.llm.predict(prompts, *args, **kwargs)\n",
    "\n",
    "    def __call__(self, prompt: str, *args, **kwargs):\n",
    "        return self.llm(prompt, *args, **kwargs)\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "response = pne.chat(\n",
    "    messages=messages,\n",
    "    custom_llm=CustomLLM(),\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-06T19:03:47.080442300Z",
     "start_time": "2023-12-06T19:03:45.078761200Z"
    }
   },
   "id": "aa746c259a9edd4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Structured Output\n",
    "The output of LLM has strong uncertainty. Pne provide the ability to get a structured object by LLM. The following example shows that if LLM strictly returns you an array listing all provinces in China. Here we use Pydantic to build a structured object."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f82e8d518c46350"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provinces=['Anhui', 'Fujian', 'Gansu', 'Guangdong', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan', 'Hubei', 'Hunan', 'Jiangsu', 'Jiangxi', 'Jilin', 'Liaoning', 'Qinghai', 'Shaanxi', 'Shandong', 'Shanxi', 'Sichuan', 'Yunnan', 'Zhejiang', 'Guangxi', 'Inner Mongolia', 'Ningxia', 'Xinjiang', 'Tibet', 'Beijing', 'Chongqing', 'Shanghai', 'Tianjin', 'Hong Kong', 'Macau']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import promptulate as pne\n",
    "\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    provinces: List[str] = Field(description=\"All provinces in China\")\n",
    "\n",
    "\n",
    "resp: LLMResponse = pne.chat(\n",
    "    messages=\"Please tell me all provinces in China.\",\n",
    "    output_schema=LLMResponse\n",
    ")\n",
    "\n",
    "print(resp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T18:41:25.086382900Z",
     "start_time": "2024-03-30T18:41:21.153407300Z"
    }
   },
   "id": "ce7e53fa05df43e3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong', 'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan', 'Hubei', 'Hunan', 'Jiangsu', 'Jiangxi', 'Jilin', 'Liaoning', 'Ningxia', 'Qinghai', 'Shaanxi', 'Shandong', 'Shanghai', 'Shanxi', 'Sichuan', 'Tianjin', 'Tibet', 'Xinjiang', 'Yunnan', 'Zhejiang']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import promptulate as pne\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    provinces: List[str] = Field(description=\"All provinces in China\")\n",
    "\n",
    "\n",
    "response: LLMResponse = pne.chat(\n",
    "    messages=\"Please tell me all provinces in China.\",\n",
    "    output_schema=LLMResponse\n",
    ")\n",
    "print(response.provinces)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T20:26:38.324581100Z",
     "start_time": "2023-12-10T20:26:33.640080200Z"
    }
   },
   "id": "fa4efacb171d3dc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, `pne.chat()` return a LLMResponse object. The value of provinces is all provinces in China. If you are building a complex Agent project, formatting output is a necessary measure to improve system robustness. The follow example show how to use `pne.chat()` to get the weather in Shanghai tomorrow."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f8c1b99869e96ee"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_name='Shanghai' queried_date=datetime.datetime(2023, 12, 11, 13, 2, 35, 722348, tzinfo=datetime.timezone.utc)\n",
      "Shanghai\n",
      "2023-12-11 13:02:35.722348+00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import promptulate as pne\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    city_name: str = Field(description=\"city name\")\n",
    "    queried_date: datetime = Field(description=\"date of timestamp\")\n",
    "\n",
    "\n",
    "current_time = datetime.now()\n",
    "response: LLMResponse = pne.chat(\n",
    "    messages=f\"What's the temperature in Shanghai tomorrow? current time: {current_time}\",\n",
    "    output_schema=LLMResponse\n",
    ")\n",
    "print(response)\n",
    "print(response.city_name)\n",
    "print(response.queried_date)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T05:02:38.742635700Z",
     "start_time": "2023-12-11T05:02:35.722348800Z"
    }
   },
   "id": "b09f45f7524a641"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using tool\n",
    "\n",
    "The Tool feature in `pne.chat()` allows the language model to use specialized tools to assist in providing answers. For instance, when the language model recognizes the need to obtain weather information, it can invoke a predefined function for this purpose.\n",
    "\n",
    "This is facilitated by a ToolAgent, which operates within the ReAct framework. The [ReAct](https://react-lm.github.io/) framework endows the ToolAgent with the ability to reason, think, and execute tools.\n",
    "\n",
    "To illustrate, if the language model needs to find out the weather forecast for Shanghai tomorrow, it can make use of the DuckDuckGoTool through the ToolAgent to retrieve this information."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "814b3228c2da3a71"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"tool\": {\"tool_name\": \"web_search\", \"tool_params\": {\"query\": \"Weather Shanghai tomorrow\"}}, \"thought\": \"I will use the web_search tool to find the temperature in Shanghai tomorrow.\", \"final_answer\": null}\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "websearch = pne.tools.DuckDuckGoTool()\n",
    "response = pne.chat(\n",
    "    messages=\"What's the temperature in Shanghai tomorrow?\",\n",
    "    tools=[websearch]\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T18:05:46.822509100Z",
     "start_time": "2023-12-11T18:05:42.669621800Z"
    }
   },
   "id": "7640390bf6a79c07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Tool\n",
    "\n",
    "Moreover, you can customize your function easily. The follow example show how to create a custom tool and use it in `pne.chat()`. Here we also we ddg websearch to wrap the function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3f5dabeebc74620"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31;1m\u001B[1;3m[Agent] Tool Agent start...\u001B[0m\n",
      "\u001B[36;1m\u001B[1;3m[User instruction] What's the temperature in Shanghai tomorrow?\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3m[Thought] I should use the websearch tool to find the weather forecast of Shanghai tomorrow.\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3m[Action] websearch args: {'query': 'Shanghai weather forecast tomorrow'}\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3m[Observation] 25Â° / 14Â°. 1.7 mm. 7 m/s. Open hourly forecast. Updated 18:30. How often is the weather forecast updated? Forecast as PDF Forecast as SVG. Shanghai Weather Forecast. Providing a local hourly Shanghai weather forecast of rain, sun, wind, humidity and temperature. The Long-range 12 day forecast also includes detail for Shanghai weather today. Live weather reports from Shanghai weather stations and weather warnings that include risk of thunder, high UV index and forecast gales. Everything you need to know about today's weather in Shanghai, Shanghai, China. High/Low, Precipitation Chances, Sunrise/Sunset, and today's Temperature History. ä¸Šæµ· (Shanghai) â˜€ Weather forecast for 10 days, information from meteorological stations, webcams, sunrise and sunset, wind and precipitation maps for this place ... 00:00 tomorrow 01:00 tomorrow 02:00 tomorrow 03:00 tomorrow 04:00 tomorrow 05:00 tomorrow 06:00 tomorrow 07:00 tomorrow 08:00 tomorrow 09:00 tomorrow Shanghai 7 day weather forecast including weather warnings, temperature, rain, wind, visibility, humidity and UV\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m[Agent Result] The weather forecast for Shanghai tomorrow is 25Â° / 14Â° with 1.7 mm of rain and 7 m/s wind speed. The weather information is updated at 18:30 daily.\u001B[0m\n",
      "\u001B[38;5;200m\u001B[1;3m[Agent] Agent End.\u001B[0m\n",
      "The weather forecast for Shanghai tomorrow is 25Â° / 14Â° with 1.7 mm of rain and 7 m/s wind speed. The weather information is updated at 18:30 daily.\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "\n",
    "def websearch(query: str) -> str:\n",
    "    \"\"\"Search the web for the query.\n",
    "    \n",
    "    Args:\n",
    "        query(str): The query word. \n",
    "\n",
    "    Returns:\n",
    "        str: The search result.\n",
    "    \"\"\"\n",
    "    return pne.tools.DuckDuckGoTool().run(query)\n",
    "\n",
    "\n",
    "response = pne.chat(\n",
    "    messages=\"What's the temperature in Shanghai tomorrow?\",\n",
    "    tools=[websearch]\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:18:36.298372300Z",
     "start_time": "2024-03-28T13:18:23.700207100Z"
    }
   },
   "id": "6f922254b7148de3",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## chat with Plan-Execute-Reflect Agent\n",
    "\n",
    "Additionally, you can enhance the capabilities of the ToolAgent by setting enable_plan=True, which activates its ability to handle more complex issues. In the pne framework, this action triggers the AssistantAgent, which can be thought of as a planning-capable ToolAgent. Upon receiving user instructions, the AssistantAgent proactively constructs a feasible plan, executes it, and then reflects on each action post-execution. If the outcome doesn't meet the expected results, the AssistantAgent will recalibrate and re-plan accordingly.\n",
    "\n",
    "This example we need to solve the problem of \"what is the hometown of the 2024 Australia open winner?\" Here we can integrate the LangChain tools to solve the problem.\n",
    "\n",
    "> pne support all LangChain Tools, you can see [here](/modules/tools/langchain_tool_usage?id=langchain-tool-usage). Of course, it is really easy to create your own tools - see documentation [here](https://undertone0809.github.io/promptulate/#/modules/tools/custom_tool_usage?id=custom-tool) on how to do that.\n",
    "\n",
    "Firstly, we need to install necessary packages.\n",
    "```bash\n",
    "pip install langchain_community\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90298940708b2f6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use [Tavily](https://app.tavily.com/) as a search engine, which is a powerful search engine that can search for information from the web. To use Tavily, you need to get an API key from Tavily.\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"your_tavily_api_key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fdabe6674f7de34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "websearch = TavilySearchResults()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T14:17:32.977027Z",
     "start_time": "2024-03-29T14:17:32.537090300Z"
    }
   },
   "id": "1aa8a230b6465dc2",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response = pne.chat(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=\"What's the temperature in Shanghai tomorrow?\",\n",
    "    tools=[websearch],\n",
    "    enable_plan=True\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9308fb846bc8e34",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Agent] Assistant Agent start...\n",
    "[User instruction] What's the temperature in Shanghai tomorrow?\n",
    "[Plan] {\"goals\": [\"Find out the temperature in Shanghai tomorrow.\"], \"tasks\": [{\"task_id\": 1, \"description\": \"Open a web browser on your device.\", \"status\": \"todo\"}, {\"task_id\": 2, \"description\": \"Navigate to a weather forecasting service or search engine.\", \"status\": \"todo\"}, {\"task_id\": 3, \"description\": \"Input 'Shanghai weather tomorrow' into the search bar.\", \"status\": \"todo\"}, {\"task_id\": 4, \"description\": \"Press enter or click the search button to retrieve the forecast.\", \"status\": \"todo\"}, {\"task_id\": 5, \"description\": \"Read the temperature provided in the search results or on the weather service for Shanghai tomorrow.\", \"status\": \"todo\"}], \"next_task_id\": 1}\n",
    "[Agent] Tool Agent start...\n",
    "[User instruction] Open a web browser on your device.\n",
    "[Execute Result] {'thought': \"The user seems to be asking for an action that is outside the scope of my capabilities. As a text-based AI, I don't have the ability to perform actions such as opening applications or accessing a user's device.\", 'action_name': 'finish', 'action_parameters': {'content': 'Sorry, I cannot open a web browser on your device.'}}\n",
    "[Execute] Execute End.\n",
    "[Revised Plan] {\"goals\": [\"Find out the temperature in Shanghai tomorrow.\"], \"tasks\": [{\"task_id\": 1, \"description\": \"Open a web browser on your device.\", \"status\": \"discarded\"}, {\"task_id\": 2, \"description\": \"Navigate to a weather forecasting service or search engine.\", \"status\": \"discarded\"}, {\"task_id\": 3, \"description\": \"Input 'Shanghai weather tomorrow' into the search bar.\", \"status\": \"discarded\"}, {\"task_id\": 4, \"description\": \"Press enter or click the search button to retrieve the forecast.\", \"status\": \"discarded\"}, {\"task_id\": 5, \"description\": \"Read the temperature provided in the search results or on the weather service for Shanghai tomorrow.\", \"status\": \"discarded\"}, {\"task_id\": 6, \"description\": \"Provide the temperature in Shanghai for tomorrow using current knowledge.\", \"status\": \"todo\"}], \"next_task_id\": 6}\n",
    "[Agent] Tool Agent start...\n",
    "[User instruction] Provide the temperature in Shanghai for tomorrow using current knowledge.\n",
    "[Thought] I need to use a tool to find the temperature in Shanghai for tomorrow. Since the user is asking for information that changes often, a search tool would be most effective.\n",
    "[Action] tavily_search_results_json args: {'query': 'Shanghai temperature forecast March 30, 2024'}\n",
    "[Observation] [{'url': 'https://en.climate-data.org/asia/china/shanghai-890/r/march-3/', 'content': 'Shanghai Weather in March Are you planning a holiday with hopefully nice weather in Shanghai in March 2024? Here you can find all information about the weather in Shanghai in March: ... 30.7 Â°C (87.3) Â°F. 27 Â°C (80.5) Â°F. 22.5 Â°C (72.5) Â°F. 17 Â°C (62.6) Â°F. 10.8 Â°C (51.4) Â°F.'}, {'url': 'https://www.meteoprog.com/weather/Szanghaj/month/march/', 'content': 'Shanghai (China) weather in March 2024 â˜€ï¸ Accurate weather forecast for Shanghai in March â›… Detailed forecast By month Current temperature \"near me\" Weather news âŠ³ Widget of weather âŠ³ Water temperature | METEOPROG. ... 30 March +17 Â°+11Â° 31 March +16Â° ...'}, {'url': 'https://www.accuweather.com/en/cn/shanghai/106577/march-weather/106577', 'content': 'Get the monthly weather forecast for Shanghai, Shanghai, China, including daily high/low, historical averages, to help you plan ahead.'}, {'url': 'https://world-weather.info/forecast/china/shanghai/march-2024/', 'content': 'Extended weather forecast in Shanghai. Hourly Week 10 days 14 days 30 days Year. Detailed âš¡ Shanghai Weather Forecast for March 2024 - day/night ðŸŒ¡ï¸ temperatures, precipitations - World-Weather.info.'}, {'url': 'https://www.weather25.com/asia/china/shanghai?page=month&month=March', 'content': \"Our weather forecast can give you a great sense of what weather to expect in Shanghai in March 2024. If you're planning to visit Shanghai in the near future, we highly recommend that you review the 14 day weather forecast for Shanghai before you arrive. Temperatures. 59 Â° / 44 Â°. Rainy Days.\"}]\n",
    "[Thought] The search results provided URLs with general March weather information and historical data for Shanghai, but haven't provided a specific forecast for tomorrow's temperature. I need to refine my search to get the weather forecast specifically for March 30, 2024, in Shanghai.\n",
    "[Action] tavily_search_results_json args: {'query': 'Shanghai weather forecast for March 30, 2024'}\n",
    "[Observation] [{'url': 'https://www.accuweather.com/en/cn/shanghai/106577/march-weather/106577', 'content': 'Get the monthly weather forecast for Shanghai, Shanghai, China, including daily high/low, historical averages, to help you plan ahead.'}, {'url': 'https://world-weather.info/forecast/china/shanghai/march-2024/', 'content': 'Extended weather forecast in Shanghai. Hourly Week 10 days 14 days 30 days Year. Detailed âš¡ Shanghai Weather Forecast for March 2024 - day/night ðŸŒ¡ï¸ temperatures, precipitations - World-Weather.info.'}, {'url': 'https://www.weather-atlas.com/en/china/shanghai-weather-march', 'content': \"In Shanghai, China, in March, the average water temperature is 8Â°C (46.4Â°F). Swimming in 8Â°C (46.4Â°F) is considered life-threatening. Even a few minutes in 13Â°C (55.4Â°F) water is uncomfortable, and swimming below 10Â°C (50Â°F) may cause total loss of breathing control and cold shock, depending on a person's physique.\"}, {'url': 'https://www.meteoprog.com/weather/Szanghaj/month/march/', 'content': 'Shanghai (China) weather in March 2024 â˜€ï¸ Accurate weather forecast for Shanghai in March â›… Detailed forecast By month Current temperature \"near me\" Weather news âŠ³ Widget of weather âŠ³ Water temperature | METEOPROG. ... 30 March +17 Â°+11Â° 31 March +16Â° ...'}, {'url': 'https://www.weather25.com/asia/china/shanghai?page=month&month=March', 'content': \"Our weather forecast can give you a great sense of what weather to expect in Shanghai in March 2024. If you're planning to visit Shanghai in the near future, we highly recommend that you review the 14 day weather forecast for Shanghai before you arrive. Temperatures. 59 Â° / 44 Â°. Rainy Days.\"}]\n",
    "[Execute Result] {'thought': \"The search has returned a specific forecast for March 30, 2024, which indicates that the temperatures are expected to be +17 Â°C for the high and +11 Â°C for the low. This information is sufficient to answer the user's question.\", 'action_name': 'finish', 'action_parameters': {'content': 'The temperature in Shanghai for tomorrow, March 30, 2024, is expected to be a high of +17 Â°C and a low of +11 Â°C.'}}\n",
    "[Execute] Execute End.\n",
    "[Revised Plan] {\"goals\": [\"Find out the temperature in Shanghai tomorrow.\"], \"tasks\": [{\"task_id\": 6, \"description\": \"Provide the temperature in Shanghai for tomorrow using current knowledge.\", \"status\": \"done\"}], \"next_task_id\": null}\n",
    "[Agent Result] The temperature in Shanghai for tomorrow, March 30, 2024, is expected to be a high of +17 Â°C and a low of +11 Â°C.\n",
    "[Agent] Agent End.\n",
    "The temperature in Shanghai for tomorrow, March 30, 2024, is expected to be a high of +17 Â°C and a low of +11 Â°C.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42318eb25991fe0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output Formatter\n",
    "\n",
    "The output formatter is a powerful feature in pne. It can help you format the output of LLM. The follow example show how to use the output formatter to format the output of LLM."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f70b1f15cfca8ef7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "provinces=['Anhui', 'Fujian', 'Gansu', 'Guangdong', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan', 'Hubei', 'Hunan', 'Jiangsu', 'Jiangxi', 'Jilin', 'Liaoning', 'Qinghai', 'Shaanxi', 'Shandong', 'Shanxi', 'Sichuan', 'Yunnan', 'Zhejiang', 'Taiwan', 'Guangxi', 'Nei Mongol', 'Ningxia', 'Xinjiang', 'Xizang', 'Beijing', 'Chongqing', 'Shanghai', 'Tianjin', 'Hong Kong', 'Macao']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import promptulate as pne\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class LLMResponse(BaseModel):\n",
    "    provinces: List[str] = Field(description=\"List of provinces name\")\n",
    "\n",
    "\n",
    "resp: LLMResponse = pne.chat(\"Please tell me all provinces in China.?\",\n",
    "                             output_schema=LLMResponse)\n",
    "print(resp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T18:43:57.633829400Z",
     "start_time": "2024-03-30T18:43:50.813515600Z"
    }
   },
   "id": "cf7b8fc1cce1eb5f",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Streaming\n",
    "`pne.chat()` support streaming, you can use `pne.chat()` to chat with your assistant in real time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3475e266c1c31343"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'choices'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpromptulate\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpne\u001B[39;00m\n\u001B[0;32m      3\u001B[0m response \u001B[38;5;241m=\u001B[39m pne\u001B[38;5;241m.\u001B[39mchat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWho are you?\u001B[39m\u001B[38;5;124m\"\u001B[39m, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chuck \u001B[38;5;129;01min\u001B[39;00m response:\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28mprint\u001B[39m(chuck)\n",
      "File \u001B[1;32mD:\\Projects\\CogitAGI\\promptulate\\promptulate\\schema.py:110\u001B[0m, in \u001B[0;36mStreamIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;124;03mThe next method for the BaseStreamIterator class.\u001B[39;00m\n\u001B[0;32m     97\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;124;03m    otherwise it returns the content of the response as a string.\u001B[39;00m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresponse_stream:\n\u001B[1;32m--> 110\u001B[0m     message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m message \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    112\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m message\n",
      "File \u001B[1;32mD:\\Projects\\CogitAGI\\promptulate\\promptulate\\schema.py:81\u001B[0m, in \u001B[0;36mStreamIterator.parse_chunk\u001B[1;34m(self, chunk)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, chunk) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Union[\u001B[38;5;28mstr\u001B[39m, BaseMessage]]:\n\u001B[0;32m     71\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;124;03m    This method is used to parse a chunk from the response stream. It returns\u001B[39;00m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;124;03m    None if the chunk is empty, otherwise it returns the parsed chunk.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;124;03m        Optional: The parsed chunk or None if the chunk is empty.\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 81\u001B[0m     content, ret_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m content \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     83\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Projects\\CogitAGI\\promptulate\\promptulate\\chat.py:33\u001B[0m, in \u001B[0;36mparse_content\u001B[1;34m(chunk)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse_content\u001B[39m(chunk) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m (\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     25\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Parse the litellm chunk.\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124;03m        chunk: litellm chunk.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;124;03m        ret_data: The additional data of the chunk.\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m     content \u001B[38;5;241m=\u001B[39m \u001B[43mchunk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoices\u001B[49m[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdelta\u001B[38;5;241m.\u001B[39mcontent\n\u001B[0;32m     34\u001B[0m     ret_data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(chunk\u001B[38;5;241m.\u001B[39mjson())\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m content, ret_data\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'choices'"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response = pne.chat(\"Who are you?\", stream=True)\n",
    "\n",
    "for chuck in response:\n",
    "    print(chuck)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T10:38:06.833731Z",
     "start_time": "2024-04-25T10:38:01.198721Z"
    }
   },
   "id": "b2cf5a0c878dffe9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`pne.chat()` by stream will return an iterator, you can use `next()` or `for each` to get the response.\n",
    "\n",
    "If you want to get metadata, you can use `return_raw_response=True` to get the raw response wrapped by `pne.AssistantMessage`. Metadata will store in `pne.AssistantMessage.additional_kwargs`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b66f83fb7b085d15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "response = pne.chat(\"Who are you?\", stream=True, return_raw_response=True)\n",
    "\n",
    "for chuck in response:\n",
    "    print(chuck.content)\n",
    "    print(chuck.additional_kwargs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d782fb41fe96150"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AIChat\n",
    "\n",
    "If you have multi-conversation and only use one LLM, you can use `pne.AIChat` init a chat object. It will save the LLM object and you can use it to chat.\n",
    "\n",
    "The follow example show how to use `pne.AIChat` to chat."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e6c7247432bc6ac"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import promptulate as pne\n",
    "\n",
    "ai_chat = pne.AIChat(model=\"gpt-4-1106-preview\", model_config={\"temperature\": 0.5})\n",
    "resp: str = ai_chat.run(\"Hello\")\n",
    "print(resp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T18:50:32.280941300Z",
     "start_time": "2024-03-30T18:50:29.317554100Z"
    }
   },
   "id": "d4eb10e823d44623",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve && RAG\n",
    "**RAG(Retrieval-Augmented Generation)** is a important data retrieve method. You can use `pne.chat()` to retrieve data from your database.\n",
    "\n",
    "You can use lots of methods to retrieve data, pne plan to support the following source:\n",
    "- [ ] VectorStore Retrieval\n",
    "- [ ] Relational Database Retrieval\n",
    "- [ ] Web Search Retrieval\n",
    "- [ ] Knowledge Graph Retrieval\n",
    "- [ ] Document Retrieval\n",
    "    - [ ] PDF Retrieval\n",
    "    - [ ] Docx Retrieval\n",
    "    - [ ] CSV/Excel Retrieval\n",
    "- [ ] Image Retrieval\n",
    "\n",
    "ðŸŒŸ**We are currently building infrastructure, please stay tuned!**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a11f1f73fd315d79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
